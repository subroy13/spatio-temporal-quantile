\documentclass[11pt]{article}

% DON'T change margins - should be 1 inch all around.
\usepackage[margin=1in]{geometry}
\usepackage{setspace}

%% Packages
\usepackage{parskip}
\usepackage{authblk}
\usepackage{amsthm,amsmath,amsfonts,amssymb,bm}
\usepackage{graphicx,psfrag,epsf}
\usepackage{multicol,booktabs}
\usepackage{color,hyperref,xcolor}
\hypersetup{colorlinks=true,urlcolor=blue,citecolor=purple}
\usepackage{cleveref}
\usepackage{longtable}

\allowdisplaybreaks

\usepackage[authoryear]{natbib}
\bibliographystyle{plainnat}

% CUSTOM COMMANDS %
\def\E{\mathbb{E}}
\def\R{\mathbb{R}}
\def\I{\mathbf{I}}
\def\Z{\mathbb{Z}}
\def\X{\bm{X}}
\def\L{\bm{L}}
\def\Y{\bm{Y}}
\def\N{\mathbb{N}}
\def\F{\mathcal{F}}
\def\M{\mathcal{M}}

\newcommand{\bb}[1]{\boldsymbol{#1}}
\newcommand{\ind}[1]{\boldsymbol{1}\{#1\}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Fcal}{\mathcal{F}}
\renewcommand{\deg}{^{\circ}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\tr}{^{\intercal}}
\newcommand{\Kcal}{\mathcal{K}}

\newcommand{\normdist}{\mathcal{N}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\eps}{\varepsilon}
\newcommand{\given}{\hspace{1pt}\vert\hspace{1pt}}
\newcommand{\multi}{\mathrm{Multinomial}}
\newcommand{\ber}{\mathrm{Bernoulli}}
\newcommand{\dash}{^{\prime}}
\newcommand{\ddash}{^{\prime\prime}}
\DeclareMathOperator*{\argmin}{arg\,min}

% \numberwithin{equation}{section}  

\newtheoremstyle{general}
{3mm} % Space above
{3mm} % Space below
{\it} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{general}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}

\makeatletter
\renewenvironment{proof}[1][\proofname]{\par
    \pushQED{\qed}%
    \normalfont \topsep6\p@\@plus6\p@\relax
    \trivlist
    \item\relax{
        \bfseries
        #1\@addpunct{.}}\hspace\labelsep\ignorespaces
    }{%
     \popQED\endtrivlist\@endpefalse
     }
\makeatother


\title{Quantile estimation of Correlated Spatio-Temporal Process}
\author{}
\date{\today}

\numberwithin{equation}{section}

\begin{document}
\maketitle

\section{Introduction}

Quantile regression is an useful method to quantify the effect of a covariate on the distribution of response. The usual regression on the conditional mean of the response variable may be inadequate in certain applications. For example, in weather data, it is very useful to analyze the occurrence of extreme heat (or extreme cold) events, rather than the average temperature conditions. 

% NEED TO WRITE GOOD INTRODUCTION

\section{Proposed Estimator}

We consider a spatio-temporal setup similar to the one described in~\cite{deb2021nonparametric}. Let $Y_{tj}$ to be the observation at $t$-th timepoint for the $j$-th location space, for $t = 1, 2, \dots n$ and $j = 1, 2, \dots p$. Also, assume that there exists a time-dependent covariate $X_t \in \R^d$, which controls the general trend across all locations. 

\begin{equation}
    \bb{Y}_t = \begin{pmatrix}
    Y_{t1}\\ Y_{t2} \\ \vdots \\ Y_{tp}
    \end{pmatrix} = \mu(X_t) + \Sigma^{1/2}(X_t) \begin{pmatrix}
    e_{it}\\ e_{it} \\ \vdots \\ e_{tp}
    \end{pmatrix}
    \label{eqn:setup}
\end{equation}
\noindent where $\mu(\cdot): \R^d \rightarrow \R^p$ and $\Sigma(X_t)$ is a $m\times m$ positive definite matrix depending on the covariates $X_t$, and $e_{tj}$'s are errors which are independent and identically distributed. We will make the following assumption about the covariates.
\begin{enumerate}
    \item[(A1)] There exists a sequence of random variables $\{\epsilon_t\}_{t = -\infty}^{\infty}$ with natural filtration $\Fcal_t$ independent of the random variables $e_{tj}$s such that $X_t$ is $\Fcal_{(t-1)}$ measurable for any $t = 0, \pm 1, \pm 2, \dots$. That implies $X_t = h(\dots, \epsilon_{-1}, \epsilon_0, \epsilon_1, \dots \epsilon_{(t-1)})$ for a measurable function $h$.
\end{enumerate}

With such a setup as given in Eq.~\eqref{eqn:setup}, the quantity of interest would be the quantiles of the $p$-variate distribution of $\bb{Y}_t$. There have been numerous notions of quantiles for multivariate distribution using convex hull~\citep{eddy1985ordering}, minimum volume sets~\citep{einmahl1992generalized}, geometric generalization of spatial median~\citep{chaudhuri1996geometric}, center outward quantile functions~\citep{figalli2018continuity} etc. Among this, the geometric notion of quantile developed by~\cite{chaudhuri1996geometric} is a direct generalization of the usual quantile regression framework by~\cite{koenker1978regression}, and hence it also fits nicely into the spatio-temporal framework in our consideration. He defines the multivariate quantile for a $d$-variate random vector $\bb{X}$ as 
\begin{equation*}
    Q(\bb{u}) = \arg\min_{\bb{q} \in \R^d} \E\left[ \Phi(\bb{u}, \bb{X} - \bb{q}) \right],
\end{equation*}
\noindent for any $u \in B^d = \{ x \in \R^d : \Vert x \Vert < 1 \}$, where $\Phi(\bb{u}, \bb{t}) = \Vert \bb{t}\Vert + \langle \bb{u}, \bb{t}\rangle$. This notion generalizes the idea of \cite{koenker1978regression} who viewed the quantile estimation problem as an optimization problem of the pinball loss function. Note that, the choice of $\bb{u} \in B^d$ towards the boundary leads of more extreme quantiles, while the choice of $\bb{u}$ towards the origin leads to the spatial centers. Bahadur's representation and various asymptotic analysis of this geometric notion of multivariate quantiles in given by \cite{chaudhuri1996geometric}.

In the spatio-temporal setup as in Eq.~\eqref{eqn:setup}, if we consider only a single location (i.e., $p = 1$) and assume the model as only a temporal model, then a natural nonparametric quantile estimator would be 
\begin{equation}
    \arg\min_{q} \sum_{t=1}^n k(x, x_t) \left[ \vert Y_t - q\vert + u(Y_t - q) \right],
\end{equation}
\noindent where $k(\cdot, \cdot)$ is a kernel of appropriate bandwidth. For $u = (2\alpha - 1)$ with $0 < \alpha < 1$, this estimator becomes the kernel weighted version of the $\alpha$-th quantile of the conditional distribution of $Y_t$ conditioned on $x_t = x$. Therefore, a natural choice of estimator in the original spatio-temporal setup (with $p > 1$) is to introduce matrix valued kernel functions $K(\cdot, \cdot)$ (see \cite{alvarez2011kernels} for details) in the inner products. Thus, we consider the estimator 
\begin{equation}
    \widehat{Q}(x, \bb{u}) = \arg\min_{\bb{q}\in \R^d} \left[ \sum_{t=1}^n  \left\Vert \bb{Y}_t - \bb{q}\right\Vert_{K^2(x,X_t)} + \left\langle\bb{u}, \bb{Y}_t - \bb{q} \right\rangle_{K(x,X_t)} \right],    \label{eqn:estimator}
\end{equation}
\noindent where $\left\Vert \bb{Y}_t - \bb{q}\right\Vert_{K^2(x,X_t)} = \sqrt{(\bb{Y}_t - \bb{q})\tr K^2(x, X_t)(\bb{Y}_t - \bb{q}) }$ and $\left\langle\bb{u}, \bb{Y}_t - \bb{q} \right\rangle_{K(x,X_t)} = \bb{u}\tr K(x, X_t) (\bb{Y}_t - \bb{q})$. The kernel $K(x, X_t)$ is normalized in the sense that it also contains a division by $b_n$ where $b_n$ is an appropriate normalizing constant depending on the bandwidth and sample size $n$. The population version of this estimator which we are trying to obtain is the conditional quantile of $Y$ conditioned on the value of the temporal covariate $X = x$, which is given by
\begin{equation}
    Q(x, \bb{u}) = \arg\min_{\bb{q}\in \R^d} \E_{\bb{Y} \mid \bb{X} = x}\left[ \Vert \bb{Y} - q\Vert +  \langle \bb{u}, \bb{Y} - q \rangle \right].
    \label{eqn:parameter}
\end{equation}

\section{Theoretical Properties}

In this section, we wish to show that the estimator $\widehat{Q}_n(x, \bb{u})$ is consistent for its population version $Q(x, \bb{u})$ for any choice of $\bb{u} \in \mathcal{U}$ as the number of timepoints $n \rightarrow \infty$, where
\begin{equation*}
    \mathcal{U} = \left\{ \bb{u}: \bb{u}\tr \bb{u} \leq 1 \right\}
\end{equation*}
\noindent We shall also try to investigate its asymptotic non-degenerate distribution of a scaled difference $\widehat{Q}(x, \bb{u}) - Q(x, \bb{u})$. Let us also define the following notations for fixed $\bb{u}$ and fixed value of $x$,
\begin{equation*}
    \begin{split}
        \Lcal_{n}(q) & = \sum_{i=1}^n \left[ \Vert \bb{Y}_t - q\Vert_{K^2(x, X_t)} + \langle \bb{u}, \bb{Y}_t - q \rangle_{K(x,X_t)} \right]\\
        \Lcal(q) & = \E\left[ \Vert Y(x) - q\Vert + \langle \bb{u}, Y(x) - q\rangle \right]
    \end{split}
\end{equation*}
\noindent where $Y(x)$ denotes that the random variable $Y$ is a stochastic function of $x$, with a deterministic trend part and an additive random noise component. 

We will start a result similar to the Theorem 2.1.2 of \cite{chaudhuri1996geometric}, which defines the estimating equation of the estimator.

\begin{lemma}\label{lemma:estimating-eqn}
    For any $x \in \R^d$ and any $\bb{u} \in \mathcal{U}$, the estimator $\widehat{Q}(x, \bb{u})$ satisfy 
    \begin{equation}
        \sum_{t=1}^n K(x, X_t) \left[\dfrac{K(x, X_t)(\bb{Y}_t - \widehat{Q})  }{ \Vert K(x, X_t)(\bb{Y}_t - \widehat{Q}) \Vert } + \bb{u}\right] = 0.
        \label{eqn:est-equation}
    \end{equation}
    The true quantile $Q(x, \bb{u})$ satisfy the population version of the similar equation 
    \begin{equation}
        \E_{\bb{Y} \mid \bb{X} = x}\left[ \dfrac{Y- Q(x, \bb{u}) }{\Vert Y- Q(x, \bb{u}) \Vert } + \bb{u}\right] = 0.
        \label{eqn:est-equation-true}
    \end{equation}
\end{lemma}

\begin{proof}

Note that, since $K_n(x, X_t)$ is a kernel matrix, it is positive definite. Therefore, the function $\Phi_K(\bb{u}, \bb{Y}_t - q) = \Vert \bb{Y}_t - q\Vert_{K^2(x, X_t)} + \langle \bb{u}, \bb{Y}_t - q\rangle_{K(x, X_t)}$ is convex in $q$. This implies that for any $h \in \R^d$, we must have 
\begin{align*}
    & \lim_{\epsilon \rightarrow 0+} \dfrac{1}{\epsilon}\sum_{i=1}^n \Phi_K(\bb{u}, \bb{Y}_t - \widehat{Q} + \epsilon h) - \Phi_K(\bb{u}, \bb{Y}_t - \widehat{Q} ) \geq 0\\
    \implies & \sum_{t=1}^n \lim_{t \rightarrow 0+} \dfrac{\Vert \bb{Y}_t - \widehat{Q} + \epsilon h \Vert_{K^2(x, X_t)} - \Vert \bb{Y}_t - \widehat{Q} \Vert_{K^2(x, X_t)} }{ \epsilon } + \lim_{\epsilon \rightarrow 0+} \dfrac{ \langle \bb{u}, \epsilon h \rangle_{K(x,X_t)} }{\epsilon} \geq 0\\
    \implies & \sum_{t=1}^{n} \left[ \dfrac{\langle \bb{Y}_t - \widehat{Q}, h\rangle_{K^2(x, X_t)} }{ \Vert \bb{Y}_t - \widehat{Q} \Vert_{K^2(x, X_t)} } + \langle u, h\rangle_{K(x, X_t)} \right] \geq 0, \qquad \text{If } \widehat{Q} \neq Y_t \text{ for any }t = 1, 2, \dots n
\end{align*}

Since this holds for any $h \in \R^d$, we can replace $(-h)$ instead of $h$ and we obtain the equality in Eq.~\eqref{eqn:est-equation}. Eq.~\eqref{eqn:est-equation-true} follows similarly from Eq.~\eqref{eqn:parameter}.
\end{proof}

Next, we shall present a generic result.

\begin{lemma}\label{lemma:generic}
    Let, $\bb{X} \sim \normdist_p(\mu, \Sigma)$. Then 
    \begin{align*}
        \E\left( \dfrac{\bb{X} }{\Vert \bb{X} \Vert} \right) & = \dfrac{\Gamma((p-1)/2)}{\sqrt{2}\Gamma(p/2)}\dfrac{(p-1)}{p}\Sigma^{1/2}\mu + o(p^{-3/2})\\
        \E\left( \dfrac{ \bb{X}\bb{X}\tr}{\Vert \bb{X}\Vert^2} \right) & = \dfrac{1}{p}(\bb{I} + \Sigma^{1/2}\mu\mu\tr \Sigma^{1/2}) + \dfrac{\Sigma^{1/2}\mu \mu\tr \Sigma^{1/2}}{p(p-2)} + \\
        & \dfrac{(\text{trace}(\Sigma^{1/2}) - p + 1)(\text{trace}(\Sigma^{1/2}) - p + 3)}{p}\bb{I} - 2\dfrac{(\text{trace}(\Sigma^{1/2}) - p + 2)}{p}\Sigma^{1/2} + \dfrac{1}{p}\Sigma + o(p^{-2})
    \end{align*}
\end{lemma}
\begin{proof}
    Since, $\bb{X} \sim \normdist_p(\mu, \Sigma)$ we can rewrite $\bb{X} = \mu + \Sigma^{1/2} \bb{Z}$ where $\bb{Z} \sim \normdist_p(0, I)$. 
    Let us rewrite $\dfrac{\bb{X}}{\Vert \bb{X} \Vert}$ as a function of $\mu$ and $\Sigma^{1/2}$, namely $h(\mu, \Sigma^{1/2}) = \dfrac{(\mu + \Sigma^{1/2}\bb{Z})}{\Vert \mu + \Sigma^{1/2}\bb{Z} \Vert}$. Then, an application of Taylor's theorem on $h$ around the point $(0, I)$ yields that 
    \begin{align*}
        & \E\left[\dfrac{\bb{X}}{\Vert \bb{X} \Vert} \right]\\
        = \quad & \E\left[ \dfrac{\bb{Z}}{\Vert \bb{Z}\Vert} \right] + \E\left( \dfrac{I}{\Vert \bb{Z}\Vert } - \dfrac{\bb{Z}\bb{Z}\tr }{\Vert \bb{Z}\Vert^3} \right)\Sigma^{1/2}\mu + \E\left( \left\langle \dfrac{I}{\Vert \bb{Z}\Vert } - \dfrac{\bb{Z}\bb{Z}\tr }{\Vert \bb{Z}\Vert^3}, \Sigma^{1/2} - I \right\rangle \bb{Z} \right) + o(\E(\Vert \bb{Z}\Vert^{-3}))\\
        = \quad & \dfrac{\Gamma((p-1)/2)}{\sqrt{2}\Gamma(p/2)}\dfrac{(p-1)}{p}\Sigma^{1/2}\mu + (\text{trace}(\Sigma^{1/2}) - p+1) \E\left(\dfrac{\bb{Z}}{\Vert \bb{Z}\Vert}\right) - \E\left(\dfrac{(\bb{Z}\tr \Sigma^{1/2} \bb{Z}) \bb{Z}}{\Vert \bb{Z}\Vert^3} \right) + o(p^{-3/2})\\
        = \quad &  \dfrac{\Gamma((p-1)/2)}{\sqrt{2}\Gamma(p/2)}\dfrac{(p-1)}{p}\Sigma^{1/2}\mu + o(p^{-3/2}),
    \end{align*}
    \noindent since $\dfrac{\bb{Z}}{\Vert \bb{Z}\Vert}$ and $\dfrac{(\bb{Z}\tr \Sigma^{1/2} \bb{Z}) \bb{Z}}{\Vert \bb{Z}\Vert^3}$ are symmetric about the origin. We also note that
    \begin{equation*}
        \E\left[ \dfrac{\bb{ZZ}\tr}{\Vert \bb{Z}\Vert^2} \right]
        = \dfrac{1}{p}I, \text{ and, } 
        \E\left[ \dfrac{\bb{Z} (\bb{Z}\tr \bb{A}\bb{Z}) \bb{Z}\tr }{\Vert \bb{Z}\Vert^4} \right]
        = \alpha(\bb{A}, p), \ 
        \E\left[ \dfrac{\bb{Z} (\bb{Z}\tr \bb{A}\bb{Z})^2 \bb{Z}\tr }{\Vert \bb{Z}\Vert^4} \right]
        = \beta(\bb{A}, p)
    \end{equation*}

    For any general matrix $\bb{A}$, the exact form of $\alpha(\bb{A}, p)$ is difficult to obtain. However, one can establish that $\alpha(\bb{A}, p)$ lies between $\lambda_p(\bb{A})p^{-1}$ and $\lambda_1(\bb{A})p^{-1}$, where $\lambda_1 > \lambda_2 > \dots > \lambda_p$ are the eigenvalues of the matrix $\bb{A}$. Similarly, $\lambda_p(\bb{A})^2p^{-1} \leq \beta(\bb{A}, p) \leq \lambda_1(\bb{A})^2p^{-1}$.

    Now, we use the same Taylor's series expansion on $\bb{X}\bb{X}\tr / \Vert \bb{X}\Vert^2$ to obtain
    \begin{align*}
        \E\left[ \dfrac{\bb{XX}\tr }{\Vert \bb{X}\Vert^2 } \right]
        & = \E\left[ \dfrac{\bb{ZZ}\tr }{\Vert \bb{Z}\Vert^2 } \right] + 2\E\left[ \left( \dfrac{\bb{I}}{\Vert \bb{Z}\Vert} - \dfrac{\bb{ZZ}\tr}{\Vert \bb{Z}\Vert^3} \right) \Sigma^{1/2}\mu \dfrac{\bb{Z}\tr}{\Vert \bb{Z}\Vert } \right] + \\
        & \E\left[ \left( \dfrac{\bb{I}}{\Vert \bb{Z}\Vert} - \dfrac{\bb{ZZ}\tr}{\Vert \bb{Z}\Vert^3} \right) \Sigma^{1/2}\mu\mu\tr \Sigma^{1/2} \left( \dfrac{\bb{I}}{\Vert \bb{Z}\Vert} - \dfrac{\bb{ZZ}\tr}{\Vert \bb{Z}\Vert^3} \right) \right] +
        2\E\left[ \left\langle \dfrac{I}{\Vert \bb{Z}\Vert } - \dfrac{\bb{Z}\bb{Z}\tr }{\Vert \bb{Z}\Vert^3}, \Sigma^{1/2} - I \right\rangle \dfrac{\bb{ZZ}\tr}{\Vert \bb{Z}\Vert } \right] +\\
        & 2\E\left[ \left\langle \dfrac{I}{\Vert \bb{Z}\Vert } - \dfrac{\bb{Z}\bb{Z}\tr }{\Vert \bb{Z}\Vert^3}, \Sigma^{1/2} - I \right\rangle \bb{Z}\mu\tr \Sigma^{1/2} \left( \dfrac{\bb{I}}{\Vert \bb{Z}\Vert} - \dfrac{\bb{ZZ}\tr}{\Vert \bb{Z}\Vert^3} \right) \right] + \\
        & \E\left[ \left\langle \dfrac{I}{\Vert \bb{Z}\Vert } - \dfrac{\bb{Z}\bb{Z}\tr }{\Vert \bb{Z}\Vert^3}, \Sigma^{1/2} - I \right\rangle \bb{ZZ}\tr \left\langle \dfrac{I}{\Vert \bb{Z}\Vert } - \dfrac{\bb{Z}\bb{Z}\tr }{\Vert \bb{Z}\Vert^3}, \Sigma^{1/2} - I \right\rangle \right] + o(\E\Vert \bb{Z}\Vert^{-4})\\
        & = \dfrac{1}{p}\bb{I} + \dfrac{1}{p} \Sigma^{1/2}\mu\mu\tr\Sigma^{1/2} + \dfrac{\alpha(\Sigma^{1/2}\mu\mu\tr\Sigma^{1/2}, p)}{(p-2)} + 2\dfrac{(\text{trace}(\Sigma^{1/2}) - p + 1)}{p} - 2\alpha(\Sigma^{1/2}, p) + \\
        & \dfrac{(\text{trace}(\Sigma^{1/2}) - p + 1)^2}{p} - 2(\text{trace}(\Sigma^{1/2}) - p + 1)\alpha(\Sigma^{1/2}, p) + \beta(\Sigma^{1/2}, p) + o(p^{-2})\\
        & = \dfrac{1}{p}(\bb{I} + \Sigma^{1/2}\mu\mu\tr \Sigma^{1/2}) + \dfrac{\Sigma^{1/2}\mu \mu\tr \Sigma^{1/2}}{p(p-2)} + \dfrac{(\text{trace}(\Sigma^{1/2}) - p + 1)(\text{trace}(\Sigma^{1/2}) - p + 3)}{p}\bb{I}\\
        & - 2\dfrac{(\text{trace}(\Sigma^{1/2}) - p + 2)}{p}\Sigma^{1/2} + \dfrac{1}{p}\Sigma + o(p^{-2})
    \end{align*}
\end{proof}

















\bibliography{references}

\end{document}



